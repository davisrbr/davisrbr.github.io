<!DOCTYPE html>
<html>
<title>Highlighted Papers</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
<link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600&family=Lora:wght@400;500;600;700&display=swap" rel="stylesheet">
<style>
  :root {
    --primary-color: #191970;
    --accent-color: #4169e1;
    --highlight-color: #fff3b0;
    --light-bg: #f9f9f9;
    --border-color: #e0e0e0;
  }
  
  body {
    font-family: 'Lora', serif;
    line-height: 1.6;
    color: #222;
    font-size: 16px;
    font-weight: 400;
  }
  
  .mono-font {
    font-family: 'IBM Plex Mono', monospace;
    font-weight: 400;
  }
  
  a {
    color: var(--accent-color);
    transition: color 0.3s, text-decoration 0.3s;
    text-decoration: none;
  }
  
  a:hover {
    color: var(--primary-color);
    text-decoration: underline;
  }

  .sidebar {
    background-color: var(--light-bg);
    box-shadow: 0 0 10px rgba(0,0,0,0.03);
    border-right: 1px solid var(--border-color);
    position: relative;
    overflow: hidden;
  }

  .highlight {
    background-color: var(--highlight-color);
    padding: 0.2rem 0.5rem;
    border-radius: 2px;
    font-weight: 700;
    font-size: 1.25rem;
  }

  .nav-links a {
    display: block;
    padding: 8px 0;
    text-decoration: none;
    transition: transform 0.2s;
  }

  .nav-links a:hover {
    transform: translateX(5px);
  }

  .paper-item {
    margin-bottom: 0px;
    padding-bottom: 0px;
  }

  .description-toggle {
    color: var(--accent-color);
    cursor: pointer;
    font-size: 0.95rem;
    font-weight: 500;
    opacity: 0.9;
    margin-top: 5px;
    display: inline-block;
  }
  
  .description-toggle:hover {
    text-decoration: underline;
  }
  
  .paper-description {
    font-size: 0.95rem;
    margin-top: 5px;
    overflow: hidden;
    max-height: 0;
    opacity: 0;
    transition: max-height 0.3s ease-out, opacity 0.3s, padding 0.3s;
    background-color: rgba(240, 240, 240, 0.3);
    border-radius: 3px;
    padding-left: 5px;
    padding-right: 5px;
    line-height: 1.4;
  }
  
  .paper-description.visible {
    max-height: 800px;
    opacity: 1;
    padding: 10px;
  }

  .versions-container {
    margin-top: 15px;
    margin-bottom: 10px;
  }

  .nav-links a.active {
    font-weight: 700;
    color: var(--primary-color);
  }

  @media screen and (min-width: 993px) {
    .sidebar {
      position: sticky;
      top: 0;
      height: 100vh; /* Full viewport height on desktop */
      overflow-y: auto; /* In case the sidebar content is longer than viewport */
      flex: 0 0 20%; /* Make sidebar narrower (25% instead of 33.33%) */
      width: 20%; /* Explicit width setting */
    }
    
    .w3-twothird {
      flex: 0 0 80%; /* Adjust main content to fill remaining space */
      width: 80%; /* Explicit width setting */
    }
  }
  
  @media screen and (max-width: 992px) {
    .sidebar {
      height: auto !important;
      padding-bottom: 1.5rem;
    }
  }
</style>
<body class="w3-content" style="max-width:2500px">

  <div class="w3-row">
    <div class="w3-third sidebar w3-container w3-center">
      <div class="w3-padding-24">
        <h4 class="mono-font">Highlighted Papers</h4>
      </div>
      <div class="versions-container">
        <div class="mono-font" style="font-size: 1.15rem;">Versions</div>
        <div class="nav-links">
          <a href="papers.html" class="mono-font active">March '25</a>
          <a href="dec_papers.html" class="mono-font">December '24</a>
        </div>
      </div>
      <div style="margin-top: 30px;"></div>
      <div class="nav-links">
        <a href="index.html" class="mono-font">Home</a>
      </div>
    </div>
    <div class="w3-twothird w3-white w3-container">
      <div class="w3-padding-64 w3-center">
        <div class="w3-left-align w3-padding-small">
          <h3 class="mono-font">Highlighted Papers: March 2025 edition</h3>
          <p class="mono-font" style="margin-top: 0%; margin-right: 80px; font-size: 1rem;">
             Collecting research that I think is not as well known as it should be (as of 03/08/25) in either academia, government, or the AI safety community. This list is updated every few months.
          </p>

          <p class="mono-font" style="margin-right: 80px">
            <span class="highlight">
              Evals
            </span>
          </p>
          <ul style="padding-left: 20px;">
            <li class="paper-item">
              <a href="https://arxiv.org/abs/2502.03461" style="color:#191970" target="_blank">Do Large Language Model Benchmarks Test Reliability?</a>
              <br/>
              <span style="font-size: 0.9em; color: #666;">Joshua Vendrow, Edward Vendrow, Sara Beery, Aleksander Madry (2025)</span>
              <br/>
              <div class="description-toggle" data-target="desc5">
                Why this is notable
              </div>
              <div id="desc5" class="paper-description">
                Offers an operationalization of reliability for LLMs (consistently giving correct answers on existing 'saturated' benchmarks), and shows that existing benchmarks are not good at testing for this due to mislabeling and incoherent wording. Provides updated 'platinum' versions of existing benchmarks that can test for this.
              </div>
            </li>
            <li class="paper-item">
              <a href="https://arxiv.org/abs/2503.00096" style="color:#191970" target="_blank">BixBench: a Comprehensive Benchmark for LLM-based Agents in Computational Biology</a>
              <br/>
              <span style="font-size: 0.9em; color: #666;">Ludovico Mitchener, Jon M Laurent, Benjamin Tenmann, Siddharth Narayanan, Geemi P Wellawatte, Andrew White, Lorenzo Sani, Samuel G Rodriques (2025)</span>
            </li>
            <li class="paper-item">
              <a href="https://arxiv.org/abs/2409.16125" style="color:#191970" target="_blank">Analyzing Probabilistic Methods for Evaluating Agent Capabilities</a>
              <br/>
              <span style="font-size: 0.9em; color: #666;">Axel Højmark, Govind Pimpale, Arjun Panickssery, Marius Hobbhahn, Jérémy Scheurer (2024)</span>
            </li>
            <li class="paper-item">
              <a href="https://arxiv.org/abs/2412.18544" style="color:#191970" target="_blank">Consistency Checks for Language Model Forecasters</a>
              <br/>
              <span style="font-size: 0.9em; color: #666;">Daniel Paleka, Abhimanyu Pallavi Sudhir, Alejandro Alvarez, Vineeth Bhat, Adam Shen, Evan Wang, Florian Tramèr (2024)</span>
            </li>
            <li class="paper-item">
              <a href="https://www.openphilanthropy.org/request-for-proposals-improving-capability-evaluations/" style="color:#191970" target="_blank">OpenPhil RFP: Improving Capability Evaluations</a>
              <br/>
              <span style="font-size: 0.9em; color: #666;">Catherine Brewer, Alex Lawsen (2025)</span>
            </li>
          </ul>

          <p class="mono-font" style="margin-right: 80px">
            <span class="highlight">
              Science of DL
            </span>
          </p>
          <ul style="padding-left: 20px;">
            <li class="paper-item">
              <a href="https://arxiv.org/abs/2404.11534" style="color:#191970" target="_blank">Decomposing and Editing Predictions by Modeling Model Computation</a>
              <br/>
              <span style="font-size: 0.9em; color: #666;">Harshay Shah, Andrew Ilyas, Aleksander Madry (2024)</span>
              <br/>
              <div class="description-toggle" data-target="desc2">
                Why this is notable
              </div>
              <div id="desc2" class="paper-description">
                 Introduces a task for training interpretability/editing methods called <i>component modeling</i>, where a meta-model is trained to predict the effect of ablating individual model components for a single example. It would be great to see serious follow-up work attempting to scale component modeling to larger models. My guess is that despite its promise, this line of work is neglected because it's engineering-heavy and isn't fully de-risked (e.g. it's not clear how well it will scale, how to improve sampling, etc.).
              </div>
            </li>
            <li class="paper-item">
              <a href="https://arxiv.org/abs/2502.16797" style="color:#191970" target="_blank">Forecasting Rare Language Model Behaviors</a>
              <br/>
              <span style="font-size: 0.9em; color: #666;">Erik Jones, Meg Tong, Jesse Mu, Mohammed Mahfoud, Jan Leike, Roger Grosse, Jared Kaplan, William Fithian, Ethan Perez, Mrinank Sharma (2025)</span>
              <br/>
              <div class="description-toggle" data-target="desc3">
                Why this is notable
              </div>
              <div id="desc3" class="paper-description">
                Could also plausibly fall under "Elicitation" category, but I'm including in the science of DL category because of the new phenomena-- scaling laws, for elicitation / a 'most-effective jailbreak' and the generally high forecastability of elicitation and behavior. Quite excited to see how this line-of-research develops, particularly around experiments for distribution shifts at deployment time and for better, less costly, less biased methods around what they term as usefulness and correctness.
              </div>
            </li>
            <!-- <li class="paper-item">
              <a href="https://arxiv.org/abs/2502.17578" style="color:#191970" target="_blank">How Do Large Language Monkeys Get Their Power (Laws)?</a>
              <br/>
              <span style="font-size: 0.9em; color: #666;">Rylan Schaeffer, Joshua Kazdan, John Hughes, Jordan Juravsky, Sara Price, Aengus Lynch, Erik Jones, Robert Kirk, Azalia Mirhoseini, Sanmi Koyejo (2025)</span>
            </li> -->
            <li class="paper-item">
              <a href="https://arxiv.org/abs/2503.02113" style="color:#191970" target="_blank">Deep Learning is Not So Mysterious or Different</a>
              <br/>
              <span style="font-size: 0.9em; color: #666;">Andrew Gordon Wilson (2025)</span>
            </li>
          </ul>

          <p class="mono-font" style="margin-right: 80px">
            <span class="highlight">
              Scaling Laws and Compute
            </span>
          </p>
          <ul style="padding-left: 20px;">
            <li class="paper-item">
              <a href="https://epoch.ai/blog/train-once-deploy-many-ai-and-increasing-returns" style="color:#191970" target="_blank">Train Once, Deploy Many: AI and Increasing Returns</a>
              <br/>
              <span style="font-size: 0.9em; color: #666;">Ege Erdil, Tamay Besiroglu</span>
              <br/>
              <div class="description-toggle" data-target="desc4">
                Why this is notable
              </div>
              <div id="desc4" class="paper-description">
                An initial compelling argument for why AIs will likely have increasing returns to scale, over and above that attained by human workers: the ability to trade off between training and inference compute, ie "train once and deploy many." As these authors have previously noted <a href="https://epoch.ai/blog/optimally-allocating-compute-between-inference-and-training">elsewhere</a>, the strength of this effect depends on the actual technique used.
              </div>
            </li>
            <li class="paper-item">
              <a href="https://epoch.ai/data-insights/biology-models-trends" style="color:#191970" target="_blank">Biology AI models are scaling 2-4x per year after rapid growth from 2019-2021</a>
              <br/>
              <span style="font-size: 0.9em; color: #666;">Pablo Villalobos, David Atanasov (2025)</span>
            </li>
            <li class="paper-item">
              <a href="https://arxiv.org/abs/2401.00448" style="color:#191970" target="_blank">Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws</a>
              <br/>
              <span style="font-size: 0.9em; color: #666;">Nikhil Sardana, Jacob Portes, Sasha Doubov, Jonathan Frankle (2023)</span>
            </li>
          </ul>

          <p class="mono-font" style="margin-right: 80px">
            <span class="highlight">
              General Safety
            </span>
          </p>
          <ul style="padding-left: 20px;">
            <li class="paper-item">
              <a href="https://arxiv.org/abs/2406.14595" style="color:#191970" target="_blank">Adversaries Can Misuse Combinations of Safe Models</a>
              <br/>
              <span style="font-size: 0.9em; color: #666;">Erik Jones, Anca Dragan, Jacob Steinhardt (2024)</span>
            </li>
            <li class="paper-item">
              <a href="https://arxiv.org/abs/2407.02551" style="color:#191970" target="_blank">Breach By A Thousand Leaks: Unsafe Information Leakage in `Safe' AI Responses</a>
              <br/>
              <span style="font-size: 0.9em; color: #666;">David Glukhov, Ziwen Han, Ilia Shumailov, Vardan Papyan, Nicolas Papernot (2024)</span>
              <br/>
              <div class="description-toggle" data-target="descglukhov">
                Why this is notable
              </div>
              <div id="descglukhov" class="paper-description">
                This paper, and the Jones paper above it, point out a fundamental limitation of pointwise defenses: they cannot pick up on distributed harm (eg when a 'harmful' question is decomposed into multiple 'harmless' questions). These lines of work are kind of dual to the 'pointwise-undetectable' attacks on finetuning APIs, <a href="https://arxiv.org/abs/2502.14828" target="_blank">notably the recent one from Xander Davies/UK AISI </a>. This paper/Jones point out that even if an attacker does not have access to the finetuning API extracting diffuse but useful information from a model is still quite easy.
              </div>
            </li>
            <li class="paper-item">
              <a href="https://joecarlsmith.com/2025/02/19/when-should-we-worry-about-ai-power-seeking" style="color:#191970" target="_blank">When should we worry about AI power-seeking?</a>
              <br/>
              <span style="font-size: 0.9em; color: #666;">Joe Carlsmith (2025)</span>
            </li>
            <li class="paper-item">
              <a href="https://arxiv.org/abs/2501.08970" style="color:#191970" target="_blank">Multi-turn Evaluation of Anthropomorphic Behaviours in Large Language Models</a>
              <br/>
              <span style="font-size: 0.9em; color: #666;">Lujain Ibrahim, Canfer Akbulut, Rasmi Elasmar, Charvi Rastogi, Minsuk Kahng, Meredith Ringel Morris, Kevin R. McKee, Verena Rieser, Murray Shanahan, Laura Weidinger (2025)</span>
            </li>
          </ul>

          <p class="mono-font" style="margin-right: 80px">
            <span class="highlight">
              Security/Control
            </span>
          </p>
          <ul style="padding-left: 20px;">
            <!-- <li class="paper-item">
              <a href="https://arxiv.org/abs/2412.12480" style="color:#191970" target="_blank">Subversion Strategy Eval: Evaluating AI's stateless strategic capabilities against control protocols</a>
              <br/>
              <span style="font-size: 0.9em; color: #666;">Alex Mallen, Charlie Griffin, Alessandro Abate, Buck Shlegeris (2024)</span>
            </li> -->
            <li class="paper-item">
              <a href="https://github.com/UKGovernmentBEIS/control-arena" style="color:#191970" target="_blank">ControlArena (research preview)</a>
              <br/>
              <span style="font-size: 0.9em; color: #666;">UK AISI (2025)</span>
            </li>
            <li class="paper-item">
              <a href="https://redwoodresearch.substack.com/p/a-basic-systems-architecture-for" style="color:#191970" target="_blank">A basic systems architecture for AI agents that do autonomous research</a>
              <br/>
              <span style="font-size: 0.9em; color: #666;">Buck Shlegeris (2024)</span>
            </li>
            <li class="paper-item">
              <a href="https://xbow.com/blog/xbow-scoold-vuln/" style="color:#191970" target="_blank">How XBOW found a Scoold authentication bypass</a>
              <br/>
              <span style="font-size: 0.9em; color: #666;">Nico Waisman, Brendan Dolan-Gavitt (2024)</span>
            </li>
            <li class="paper-item">
              <a href="https://arxiv.org/abs/2501.08970" style="color:#191970" target="_blank">Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography</a>
              <br/>
              <span style="font-size: 0.9em; color: #666;">Ilia Shumailov, Daniel Ramage, Sarah Meiklejohn, Peter Kairouz, Florian Hartmann, Borja Balle, Eugene Bagdasarian (2025)</span>
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <!-- JavaScript -->
  <script>
    // Toggle Paper Descriptions
    document.querySelectorAll('.description-toggle').forEach(toggle => {
      toggle.addEventListener('click', function() {
        const targetId = this.getAttribute('data-target');
        const descElement = document.getElementById(targetId);
        
        descElement.classList.toggle('visible');
        
        // Update toggle text
        if (descElement.classList.contains('visible')) {
          this.textContent = 'Hide details';
        } else {
          this.textContent = 'Why this is notable';
        }
      });
    });
  </script>
</body>
</html>
